diff --git a/main.py b/main.py
index 61edcb2..5569b59 100644
--- a/main.py
+++ b/main.py
@@ -200,7 +200,7 @@ def main():
     sample_env = gym.make(args.env)
     sample_env.initialize_environment(args.state_rep, args.n_historical_events, args.episode_length, 1, args.length_expert_TS, args.agent_seed)
     demonstrations = sample_env.generate_expert_trajectories(args.n_experts, out_dir=dst, seed=args.seed_expert, eval=False)
-    timestep_limit = sample_env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')  # This value is None
+    timestep_limit = None #sample_env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')  # This value is None
 
 
     # Generate expert data for evaluation
diff --git a/main_stable_baselines.py b/main_stable_baselines.py
index ffc407d..4b1ccb8 100644
--- a/main_stable_baselines.py
+++ b/main_stable_baselines.py
@@ -20,9 +20,9 @@ def main():
     dataset = ExpertDataset(expert_path=path, traj_limitation=10, verbose=1)
     env = gym.make(env_name)
     env.initialize_environment(case = 21, n_historical_events = 96, episode_length = 512, n_demos_per_expert=1, n_expert_time_steps=256, agent_seed=0)
-    model = GAIL('MlpPolicy', env, dataset, verbose=1, full_tensorboard_log=True)
+    model = GAIL('MlpPolicy', env, dataset, verbose=1, tensorboard_log="./log_tensorboard/")
     # Note: in practice, you need to train for 1M steps to have a working policy
-    model.learn(total_timesteps=10**7)
+    model.learn(total_timesteps=10**8)
     model.save("gail")
 
     del model # remove to demonstrate saving and loading
diff --git a/stable-baselines-test/eval_expert_trajectories.npz b/stable-baselines-test/eval_expert_trajectories.npz
index aa933f0..24d54b5 100644
Binary files a/stable-baselines-test/eval_expert_trajectories.npz and b/stable-baselines-test/eval_expert_trajectories.npz differ
